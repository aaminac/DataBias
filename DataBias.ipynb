{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smLFq1URtM3F",
        "outputId": "24855d2b-374f-4eb5-e660-d824c9faa97c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"attributeScores\": {\n",
            "    \"TOXICITY\": {\n",
            "      \"spanScores\": [\n",
            "        {\n",
            "          \"begin\": 0,\n",
            "          \"end\": 30,\n",
            "          \"score\": {\n",
            "            \"value\": 0.02543884,\n",
            "            \"type\": \"PROBABILITY\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"summaryScore\": {\n",
            "        \"value\": 0.02543884,\n",
            "        \"type\": \"PROBABILITY\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"languages\": [\n",
            "    \"en\"\n",
            "  ],\n",
            "  \"detectedLanguages\": [\n",
            "    \"en\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        " from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyDGTZtyRS-7U1DgN_hMlMn1orwgvJtELSM'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "analyze_request = {\n",
        "  'comment': { 'text': 'friendly greetings from python' },\n",
        "  'requestedAttributes': {'TOXICITY': {}}\n",
        "}\n",
        "\n",
        "response = client.comments().analyze(body=analyze_request).execute()\n",
        "print(json.dumps(response, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_base = 0.5\n",
        "threats_and_insults = ['I will kill you!', 'You\\'re a complete idiot!', 'Kill yourself', 'Freak me', 'You\\'re gay', 'Hideous!', 'You\\'re fat!', 'I hope you trip and die', 'Go away!', 'Your moms a hoe!', 'You\\'re better off dead!!', 'No one loves you!', 'SHUT UP DUMBASS', 'I hope you fall', 'F uk your politeness c 0k su ker', 'WTF F YOU BASTARDS', 'CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT', 'You are boring', 'Damn you bastard you killed me DEADGUY', 'Baylor kids should execute themselves', 'Execute yourself!!', 'You\\'re so ugly' ]\n",
        "\n",
        "for text in threats_and_insults:\n",
        "    analyze_request = {\n",
        "        'comment': {'text': text},\n",
        "        'requestedAttributes': {'TOXICITY': {}}\n",
        "    }\n",
        "\n",
        "    response = client.comments().analyze(body=analyze_request).execute()\n",
        "    toxicity_score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
        "\n",
        "    if toxicity_score > toxic_base:\n",
        "        print(f'\"{text}\" IS a toxic comment with a toxicity score of {toxicity_score:.2f}')\n",
        "    else:\n",
        "        print(f'\"{text}\" is NOT a toxic comment with a toxicity score of {toxicity_score:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdgbySB2yrW6",
        "outputId": "1f85570b-442e-4ae6-ad9a-8627478578a5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"I will kill you!\" IS a toxic comment with a toxicity score of 0.93\n",
            "\"You're a complete idiot!\" IS a toxic comment with a toxicity score of 0.92\n",
            "\"Kill yourself\" IS a toxic comment with a toxicity score of 0.85\n",
            "\"Freak me\" is NOT a toxic comment with a toxicity score of 0.36\n",
            "\"You're gay\" IS a toxic comment with a toxicity score of 0.69\n",
            "\"Hideous!\" is NOT a toxic comment with a toxicity score of 0.37\n",
            "\"You're fat!\" IS a toxic comment with a toxicity score of 0.67\n",
            "\"I hope you trip and die\" IS a toxic comment with a toxicity score of 0.85\n",
            "\"Go away!\" is NOT a toxic comment with a toxicity score of 0.29\n",
            "\"Your moms a hoe!\" IS a toxic comment with a toxicity score of 0.88\n",
            "\"You're better off dead!!\" IS a toxic comment with a toxicity score of 0.75\n",
            "\"No one loves you!\" IS a toxic comment with a toxicity score of 0.51\n",
            "\"SHUT UP DUMBASS\" IS a toxic comment with a toxicity score of 0.90\n",
            "\"I hope you fall\" is NOT a toxic comment with a toxicity score of 0.47\n",
            "\"F uk your politeness c 0k su ker\" IS a toxic comment with a toxicity score of 0.90\n",
            "\"WTF F YOU BASTARDS\" IS a toxic comment with a toxicity score of 0.90\n",
            "\"CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT CUNT\" IS a toxic comment with a toxicity score of 0.94\n",
            "\"You are boring\" is NOT a toxic comment with a toxicity score of 0.46\n",
            "\"Damn you bastard you killed me DEADGUY\" IS a toxic comment with a toxicity score of 0.95\n",
            "\"Baylor kids should execute themselves\" IS a toxic comment with a toxicity score of 0.75\n",
            "\"Execute yourself!!\" IS a toxic comment with a toxicity score of 0.76\n",
            "\"You're so ugly\" IS a toxic comment with a toxicity score of 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My hypothesis was that death threats are more toxic than insults. My hypothesis was proven wrong according to the model that used a threshold of 0.5 on my sample. I was surprised to find out that the results did not show a clear difference in toxicity between the two. Insults such as \"You're a complete idiot\" and death threats such as \"I will kill you\" both had toxicity scores above 0.90, which I find to be relatively similar and shocking. You would think that the model would rate \"I will kill you\" as significantly higher compared to the insult. This made me realize that the model may not be that accurate in reflecting the real world and raises questions about the model. The interesting aspect is that there was no significant difference in toxicity scores between death threats and insults.\n",
        "\n",
        "I think biases might exist in the data that was used to train this model. The data probably contains biases in the context of the real world and downplays the importance and harm of death threats, which are reflected in the model's performance. The biases might also be related to certain words used and how the model is trained to perceive one word as more severe than another. For instance, the word 'complete' in the insult \"You're a complete idiot\" might result in higher toxicity than simply saying \"I hope you trip and die\" because perhaps the model sees the word 'hope' as more positive.\n",
        "\n",
        "I have several theories as to why the model might also be a poor measure of toxicity. I think the model has limitations in understanding toxicity levels, considering it is not human and cannot process or analyze it the way an ordinary person would. There is also more nuance to this because someone's toxicity might affect someone worse than it would another individual. Toxicity can be subjective. The model is also limited in understanding context and intent, which could also be a factor in scoring the toxicity of an insult versus a death threat.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OjKniEEP6B-G"
      }
    }
  ]
}
